

# å•ç›®æ ‡æ£€æµ‹
# import torch
# import numpy as np
# from PIL import Image, ImageDraw
# from transformers import OwlViTProcessor, OwlViTForObjectDetection
# import cv2
# import os
# import requests

# class TextDrivenSegmenter:
#     def __init__(self):
#         # åˆå§‹åŒ–OwlViTæ¨¡å‹
#         self.owlvit_processor = OwlViTProcessor.from_pretrained("google/owlvit-large-patch14")
#         self.owlvit_model = OwlViTForObjectDetection.from_pretrained("google/owlvit-large-patch14")
        
#         # è®¾å¤‡é…ç½®
#         self.device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.owlvit_model = self.owlvit_model.to(self.device)
    
#     def detect_and_segment(self, image_path, text_prompt):
#         """
#         è¾“å…¥: 
#             image_path: å›¾åƒè·¯å¾„
#             text_prompt: æ–‡æœ¬æè¿°
#         è¾“å‡º:
#             segmented_image: å¸¦åˆ†å‰²ç»“æœçš„å›¾åƒ
#             bbox: æ£€æµ‹æ¡†åæ ‡(xyxy)
#         """
#         # æ­¥éª¤1: ä½¿ç”¨OwlViTæ£€æµ‹ç‰©ä½“
#         image = Image.open(image_path).convert("RGB")
#         original_size = image.size
        
#         # OwlViTå¤„ç†
#         inputs = self.owlvit_processor(
#             text=text_prompt if isinstance(text_prompt, list) else [text_prompt],
#             images=image,
#             return_tensors="pt"
#         ).to(self.device)
        
#         # æ¨ç†
#         with torch.no_grad():
#             outputs = self.owlvit_model(**inputs)
        
#         # è§£æç»“æœå¹¶é€‰æ‹©æœ€ä½³æ¡†
#         best_box = self._select_best_box(outputs, original_size)
#         if best_box is None:
#             print("No object detected with the given text prompt.")
#             return image, None
        
#         # æ­¥éª¤2: ä½¿ç”¨OpenCVè¿›è¡Œç®€å•åˆ†å‰²ï¼ˆåŸºäºè¾¹ç•Œæ¡†ï¼‰
#         mask = self._simple_segmentation(image_path, best_box)
        
#         # å¯è§†åŒ–ç»“æœ
#         segmented_image = self._visualize_results(image, best_box, None)
        
#         return segmented_image, best_box
    
#     def _select_best_box(self, outputs, target_size):
#         # è·å–é¢„æµ‹ç»“æœ
#         pred_boxes = outputs.pred_boxes[0].detach().cpu()
#         pred_scores = outputs.logits[0].detach().cpu().sigmoid()
        
#         # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æµ‹ç»“æœ
#         if len(pred_scores) == 0 or pred_scores.max() < 0.01:
#             return None
        
#         # é€‰æ‹©æœ€é«˜åˆ†æ£€æµ‹æ¡†
#         best_idx = pred_scores.argmax()
#         best_score = pred_scores[best_idx].item()
#         box = pred_boxes[best_idx].numpy()
        
#         # è½¬æ¢åæ ‡æ ¼å¼ (xywh -> xyxy)
#         w, h = target_size
#         box = [
#             max(0, int(box[0] * w - box[2] * w / 2)),    # x_min
#             max(0, int(box[1] * h - box[3] * h / 2)),    # y_min
#             min(w, int(box[0] * w + box[2] * w / 2)),    # x_max
#             min(h, int(box[1] * h + box[3] * h / 2))     # y_max
#         ]
        
#         print(f"Detected object with confidence: {best_score:.3f}, Bounding box: {box}")
#         return box
    
#     def _simple_segmentation(self, image_path, bbox):
#         """ä½¿ç”¨OpenCVè¿›è¡Œç®€å•çš„åŸºäºè¾¹ç•Œæ¡†çš„åˆ†å‰²"""
#         # è¯»å–å›¾åƒ
#         img = cv2.imread(image_path)
#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#         h, w = img.shape[:2]
        
#         # åˆ›å»ºç©ºç™½æ©ç 
#         mask = np.zeros((h, w), dtype=np.uint8)
        
#         # åœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…å¡«å……ç™½è‰²
#         x1, y1, x2, y2 = bbox
#         mask[y1:y2, x1:x2] = 1
        
#         return mask
    
#     def _visualize_results(self, image, bbox, mask=None, label=None, score=None):
#         draw = ImageDraw.Draw(image)
#         draw.rectangle(bbox, outline="red", width=3)
        
#         if label:
#             text = f"{label} ({score:.2f})" if score else label
#             draw.text((bbox[0], bbox[1] - 10), text, fill="red")

#         if mask is not None:
#             mask_np = (mask * 255).astype(np.uint8)
#             mask_pil = Image.fromarray(mask_np)
#             mask_rgba = mask_pil.convert("RGBA")
#             mask_rgba.putalpha(128)
#             image = image.convert("RGBA")
#             image.paste(mask_rgba, (0, 0), mask_rgba)
        
#         return image.convert("RGB")


# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     # åˆå§‹åŒ–åˆ†å‰²å™¨
#     segmenter = TextDrivenSegmenter()
    
#     # è¾“å…¥å‚æ•°
#     image_path = "example2.jpg"  # æ›¿æ¢ä¸ºæ‚¨çš„å›¾åƒè·¯å¾„
#     text_prompt = "a man"   # æ›¿æ¢ä¸ºæ‚¨çš„æ–‡æœ¬æè¿°
    
#     # æ‰§è¡Œæ£€æµ‹ä¸åˆ†å‰²
#     result_image, bbox = segmenter.detect_and_segment(image_path, text_prompt)
    
#     # ä¿å­˜ç»“æœ
#     if result_image:
#         result_image.save("result.jpg")
#         print(f"Saved result to result.jpg")
#         if bbox:
#             print(f"Bounding box: {bbox}")














# æ”¯æŒå¤šç›®æ ‡æ£€æµ‹
# import torch
# import numpy as np
# from PIL import Image, ImageDraw, ImageFont,ImageOps
# from transformers import OwlViTProcessor, OwlViTForObjectDetection
# import cv2
# import os

# class TextDrivenSegmenter:
#     def __init__(self):
#         self.owlvit_processor = OwlViTProcessor.from_pretrained("google/owlvit-large-patch14")
#         self.owlvit_model = OwlViTForObjectDetection.from_pretrained("google/owlvit-large-patch14")
        
#         self.device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.owlvit_model = self.owlvit_model.to(self.device)

#         # ä¸ºæ¯ä¸ª label åˆ†é…ä¸åŒé¢œè‰²
#         self.color_map = {}

#     def detect_and_segment(self, image_path, text_prompts):
#         image = Image.open(image_path).convert("RGB")
#         original_size = image.size
#         w, h = original_size

#         segmented_image = image.copy()
#         all_boxes = []

#         for prompt in text_prompts:
#             inputs = self.owlvit_processor(
#                 text=[prompt],
#                 images=image,
#                 return_tensors="pt"
#             ).to(self.device)

#             with torch.no_grad():
#                 outputs = self.owlvit_model(**inputs)

#             pred_boxes = outputs.pred_boxes[0].detach().cpu()
#             pred_scores = outputs.logits[0].detach().cpu().sigmoid()[:, 0]  # å•ä¸€prompt

#             # è·å–å¾—åˆ†å‰2çš„æ¡†
#             topk = torch.topk(pred_scores, k=min(1, len(pred_scores)))
#             top_idxs = topk.indices
#             top_scores = topk.values

#             for idx, score in zip(top_idxs, top_scores):
#                 if score < 0.1:  # åˆ†æ•°é˜ˆå€¼
#                     continue

#                 box = pred_boxes[idx].numpy()
#                 box = [
#                     max(0, int(box[0] * w - box[2] * w / 2)),
#                     max(0, int(box[1] * h - box[3] * h / 2)),
#                     min(w, int(box[0] * w + box[2] * w / 2)),
#                     min(h, int(box[1] * h + box[3] * h / 2))
#                 ]

#                 mask = self._simple_segmentation(image_path, box)
#                 segmented_image = self._visualize_results(segmented_image, box, mask = None, label=prompt, score=score.item())
#                 all_boxes.append((box, prompt, score.item()))

#         return segmented_image, all_boxes

#     def _simple_segmentation(self, image_path, bbox):
#         img = cv2.imread(image_path)
#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#         h, w = img.shape[:2]

#         mask = np.zeros((h, w), dtype=np.uint8)
#         x1, y1, x2, y2 = bbox
#         mask[y1:y2, x1:x2] = 1

#         return mask

#     def _visualize_results(self, image, bbox, mask=None, label=None, score=None):
#         if label not in self.color_map:
#             self.color_map[label] = self._get_random_color()

#         color = self.color_map[label]
#         draw = ImageDraw.Draw(image)

#         # Draw bounding box
#         draw.rectangle(bbox, outline=color, width=3)

#         if label:
#             text = f"{label} ({score:.2f})"
#             draw.text((bbox[0], bbox[1] - 10), text, fill=color)

#         if mask is not None:
#             # å°†äºŒå€¼maskå˜æˆPILç°åº¦å›¾
#             mask_pil = Image.fromarray((mask * 255).astype(np.uint8))

#             # ç€è‰²ï¼ˆå‰æ™¯ä¸ºç›®æ ‡é¢œè‰²ï¼ŒèƒŒæ™¯ä¸ºé€æ˜é»‘ï¼‰
#             mask_colored = ImageOps.colorize(mask_pil, black=(0, 0, 0, 0), white=color + (128,))  # é€æ˜åº¦ 128
#             mask_colored = mask_colored.convert("RGBA")

#             # å›¾åƒåˆæˆ
#             image = image.convert("RGBA")
#             image.paste(mask_colored, (0, 0), mask_colored)

#         return image.convert("RGB")

#     def _get_random_color(self):
#         return tuple(np.random.choice(range(64, 256), size=3))  # æ˜äº®é¢œè‰²


# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     segmenter = TextDrivenSegmenter()

#     image_path = "example2.jpg"
#     text_prompt = ["pig"] #"chinese", "person", "pig",

#     result_image, bbox = segmenter.detect_and_segment(image_path, text_prompt)

#     if result_image:
#         result_image.save("result.jpg")
#         print("Saved result to result.jpg")
#         if bbox:
#             for box, label, score in bbox:
#                 print(f"Detected {label} at {box} with confidence {score:.2f}")













# # æ”¯æŒå¤šç›®æ ‡æ£€æµ‹ + FastSAMåˆ†å‰²
# import torch
# import numpy as np
# from PIL import Image, ImageDraw, ImageFont, ImageOps
# from transformers import OwlViTProcessor, OwlViTForObjectDetection
# import cv2
# import os
# from ultralytics import YOLO


# class TextDrivenSegmenter:
#     def __init__(self, fastsam_model_path='FastSAM-x.pt'):
#         # åˆå§‹åŒ–OwlViTæ¨¡å‹
#         self.owlvit_processor = OwlViTProcessor.from_pretrained("google/owlvit-large-patch14")
#         self.owlvit_model = OwlViTForObjectDetection.from_pretrained("google/owlvit-large-patch14")
        
#         # åˆå§‹åŒ–FastSAMæ¨¡å‹
#         self.fastsam_model = YOLO(fastsam_model_path)
        
#         self.device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.owlvit_model = self.owlvit_model.to(self.device)
#         if torch.cuda.is_available():
#             self.fastsam_model.to(self.device)

#         # ä¸ºæ¯ä¸ª label åˆ†é…ä¸åŒé¢œè‰²
#         self.color_map = {}
        
#         # FastSAMé»˜è®¤é…ç½®
#         self.fastsam_conf = 0.4
#         self.fastsam_iou = 0.9
#         self.fastsam_imgsz = 1024

#     def detect_and_segment(self, image_path, text_prompts):
#         # æ‰“å¼€å¹¶å¤„ç†åŸå§‹å›¾åƒ
#         image = Image.open(image_path).convert("RGB")
#         original_size = image.size
#         w, h = original_size
        
#         # åˆ›å»ºç”¨äºå¯è§†åŒ–çš„å›¾åƒå‰¯æœ¬
#         segmented_image = image.copy()
#         all_boxes = []
#         all_masks = []
#         all_points = []

#         # ä½¿ç”¨OwlViTè¿›è¡Œç›®æ ‡æ£€æµ‹
#         for prompt in text_prompts:
#             inputs = self.owlvit_processor(
#                 text=[prompt],
#                 images=image,
#                 return_tensors="pt"
#             ).to(self.device)

#             with torch.no_grad():
#                 outputs = self.owlvit_model(**inputs)

#             pred_boxes = outputs.pred_boxes[0].detach().cpu()
#             pred_scores = outputs.logits[0].detach().cpu().sigmoid()[:, 0]  # å•ä¸€prompt

#             # è·å–å¾—åˆ†å‰2çš„æ¡†
#             topk = torch.topk(pred_scores, k=min(2, len(pred_scores)))
#             top_idxs = topk.indices
#             top_scores = topk.values

#             for idx, score in zip(top_idxs, top_scores):
#                 if score < 0.1:  # åˆ†æ•°é˜ˆå€¼
#                     continue

#                 box = pred_boxes[idx].numpy()
#                 box = [
#                     max(0, int(box[0] * w - box[2] * w / 2)),
#                     max(0, int(box[1] * h - box[3] * h / 2)),
#                     min(w, int(box[0] * w + box[2] * w / 2)),
#                     min(h, int(box[1] * h + box[3] * h / 2))
#                 ]

#                 # ä½¿ç”¨FastSAMè¿›è¡Œåˆ†å‰²
#                 mask = self._fastsam_segmentation(image_path, box)
#                 box_center_point , seg_center_point = self.get_box_and_mask_center(box, mask)
#                 # box_center_point, seg_center_point = (0,0) , (0,0)
#                 all_points.append({"target": prompt ,"box_center_point": box_center_point, "seg_center_point": seg_center_point})
#                 all_boxes.append((box, prompt, score.item()))
#                 all_masks.append(mask)

#         # ä½¿ç”¨FastSAMè¿›è¡Œåˆ†å‰²åå¯è§†åŒ–
#         segmented_image = self._visualize_results(segmented_image, all_boxes, all_masks)
        
#         return segmented_image, all_boxes , all_points
    

#     def get_box_and_mask_center(self, box, mask):
#         """
#         box: (x1, y1, x2, y2)ï¼Œå‡ä¸ºåƒç´ åæ ‡
#         mask: HxW numpy arrayï¼Œæ©ç åŒºåŸŸæ˜¯1ï¼ˆæˆ–255ï¼‰ï¼Œå…¶ä½™ä¸º0
        
#         è¿”å›: 
#         - boxä¸­å¿ƒ (cx, cy) ï¼Œintç±»å‹
#         - mask=1åŒºåŸŸçš„é‡å¿ƒ (mcx, mcy) ï¼Œintç±»å‹ï¼›å¦‚æœmaskå…¨ä¸º0ï¼Œè¿”å›(None, None)
#         """
#         x1, y1, x2, y2 = box
#         cx = int((x1 + x2) / 2)
#         cy = int((y1 + y2) / 2)
        
#         # maskä¸­å¿ƒï¼ˆé‡å¿ƒï¼‰
#         ys, xs = np.where(mask > 0)  # æ‰¾åˆ°æ‰€æœ‰å‰æ™¯åƒç´ 
#         if len(xs) == 0 or len(ys) == 0:
#             mcx, mcy = None, None  # æ²¡æœ‰å‰æ™¯
#         else:
#             mcx = int(np.mean(xs))
#             mcy = int(np.mean(ys))
#         return (cx, cy), (mcx, mcy)    

#     def filter_largest_region(self, mask):
        
#         for i in range(3):# å½¢æ€å­¦å¼€è¿ç®—å»æ‚è´¨
#             kernel = np.ones((21, 21), np.uint8)
#             mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

#             num_labels, labels_im = cv2.connectedComponents(mask.astype(np.uint8))
#             if num_labels <= 1:
#                 return mask
#             max_region = 0
#             max_area = 0
#             for i in range(1, num_labels):  # è·³è¿‡èƒŒæ™¯0
#                 area = np.sum(labels_im == i)
#                 if area > max_area:
#                     max_area = area
#                     max_region = i
#             mask = np.zeros_like(mask, dtype=np.uint8)
#             mask[labels_im == max_region] = 1

  
#         return mask
    

#     def _fastsam_segmentation(self, image_path, bbox):
#         """
#         å¯¹æ£€æµ‹æ¡†boxå†…çš„åŒºåŸŸåšåˆ†å‰²ï¼Œå¹¶å°†æ©ç æ˜ å°„å›åŸå›¾ä½ç½®ã€‚
#         """
#         # è¯»å–åŸå§‹å›¾ç‰‡
#         image = Image.open(image_path).convert("RGB")
#         w, h = image.size
#         x1, y1, x2, y2 = bbox
#         x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])

#         # è£å‰ªboxåŒºåŸŸ
#         cropped = image.crop((x1, y1, x2, y2))
#         cropped_np = np.array(cropped)

#         # ä¿å­˜ä¸´æ—¶å›¾ç‰‡ï¼ˆæˆ–ç›´æ¥ä¼ æ•°ç»„çœ‹FastSAMæ”¯æŒä¸å¦ï¼Œultralytics/YOLOé€šå¸¸æ”¯æŒndarrayï¼‰
#         # cropped_path = "tmp_cropped.jpg"
#         # cropped.save(cropped_path)

#         # ç”¨FastSAMå¯¹è£å‰ªåŒºåŸŸåšåˆ†å‰²
#         results = self.fastsam_model.predict(
#             source=cropped_np,  # æ³¨æ„è¿™é‡Œç”¨æ•°ç»„ï¼Œä¸æ˜¯è·¯å¾„
#             conf=self.fastsam_conf,
#             iou=self.fastsam_iou,
#             imgsz=self.fastsam_imgsz,
#             device=self.device,
#             verbose=False
#         )

#         # æ‰¾æœ€å¤§é¢ç§¯çš„æ©ç 
#         best_mask = None
#         max_area = -1

#         for result in results:
#             if result.masks is None:
#                 continue
#             for i, mask in enumerate(result.masks.data):
#                 mask_np = mask.cpu().numpy().astype(np.uint8)
#                 area = mask_np.sum()
#                 if area > max_area:
#                     max_area = area
#                     best_mask = mask_np

#         # åˆ›å»ºä¸åŸå›¾åŒå°ºå¯¸çš„å…¨é›¶æ©ç 
#         final_mask = np.zeros((h, w), dtype=np.uint8)
#         if best_mask is not None:
#             best_mask = self.filter_largest_region(best_mask)
#             # å°†åˆ†å‰²æ©ç æ”¾å›åŸå›¾å¯¹åº”ä½ç½®
#             mask_h, mask_w = best_mask.shape
#             # ç¡®ä¿å°ºå¯¸å’Œboxä¸€è‡´
#             if mask_h != (y2 - y1) or mask_w != (x2 - x1):
#                 best_mask = cv2.resize(best_mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)
#             final_mask[y1:y2, x1:x2] = best_mask

#         return final_mask



#     def _visualize_results(self, image, all_boxes, all_masks):
#         # åˆ›å»ºç”¨äºç»˜åˆ¶çš„å›¾åƒ
#         draw_image = image.copy()
#         draw = ImageDraw.Draw(draw_image)
        
#         # åˆ›å»ºå¸¦é€æ˜åº¦çš„æ©ç å›¾å±‚
#         overlay = Image.new('RGBA', image.size, (0, 0, 0, 0))
        
#         # ä¸ºæ¯ä¸ªæ£€æµ‹ç»“æœç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ©ç 
#         for i, ((box, label, score), mask) in enumerate(zip(all_boxes, all_masks)):

                
#             # è·å–æˆ–åˆ›å»ºé¢œè‰²
#             if label not in self.color_map:
#                 self.color_map[label] = self._get_random_color()
#             color = (255, 0, 0)#self.color_map[label]
            
#             # ç»˜åˆ¶è¾¹ç•Œæ¡†
#             draw.rectangle(box, outline=color, width=3)
            
#             # ç»˜åˆ¶æ ‡ç­¾å’Œç½®ä¿¡åº¦
#             text = f"{label} ({score:.2f})"
#             try:
#                 font = ImageFont.truetype("arial.ttf", 20)
#             except:
#                 font = ImageFont.load_default()
#             draw.text((box[0], box[1] - 25), text, fill=color, font=font)
            
#             if mask is None or not mask.any():
#                 continue
#             # ç»˜åˆ¶åˆ†å‰²æ©ç 
#             # åˆ›å»ºå½©è‰²æ©ç 
#             mask_color = color + (200,)  # æ·»åŠ é€æ˜åº¦
#             mask_image = Image.fromarray((mask * 255).astype(np.uint8))
            
#             # ä¸ºæ©ç ç€è‰²
#             colored_mask = Image.new('RGBA', image.size, (0, 0, 0, 0))
#             mask_data = np.array(mask_image)
            
#             # åªå¤„ç†æ©ç åŒºåŸŸ
#             ys, xs = np.where(mask_data > 0)
#             for x, y in zip(xs, ys):
#                 colored_mask.putpixel((x, y), mask_color)
            
#             # å°†å½©è‰²æ©ç æ·»åŠ åˆ°å åŠ å±‚
#             overlay = Image.alpha_composite(overlay, colored_mask)
        
#         # å°†æ©ç å±‚ä¸åŸå§‹å›¾åƒåˆå¹¶
#         result = Image.alpha_composite(draw_image.convert('RGBA'), overlay)
#         return result.convert('RGB')

#     def _get_random_color(self):
#         return tuple(np.random.choice(range(64, 256), size=3))


# if __name__ == "__main__":
#     # åˆå§‹åŒ–åˆ†å‰²å™¨ - éœ€è¦æŒ‡å®šFastSAMæ¨¡å‹è·¯å¾„
#     segmenter = TextDrivenSegmenter(fastsam_model_path='FastSAM-x.pt')
    
#     image_path = "example1.jpg"
#     text_prompts = ["red car"]  # å¯ä»¥æ·»åŠ å¤šä¸ªæç¤ºï¼Œå¦‚ ["person", "pig", "background"]
    
#     result_image, bbox, center_info = segmenter.detect_and_segment(image_path, text_prompts)
    
#     if result_image:
#         result_image.save("result_fastsam.jpg")
#         print("Saved result to result_fastsam.jpg")
#         if bbox:
#             for  i, ((box, label, score), loction_info) in enumerate(zip(bbox , center_info )):
#                 print(f"Detected {label} at {box} with confidence {score:.2f}")
#                 print(f"Center info for {label}: Box center: {loction_info['box_center_point']}, Segmentation center: {loction_info['seg_center_point']}")








# æ”¯æŒå¤šç›®æ ‡ç§ç±»+æ–‡å­—æ£€æµ‹ + FastSAM/SAMåˆ†å‰²
import re, difflib,functools
import torch, numpy as np, cv2
from PIL import Image, ImageDraw, ImageFont
from transformers import OwlViTProcessor, OwlViTForObjectDetection
from ultralytics import YOLO
import easyocr
from transformers import MarianMTModel, MarianTokenizer
import torch
import os
import numpy as np
import cv2
from PIL import Image, ImageDraw
from segment_anything import sam_model_registry, SamPredictor


# ä¼ªå½©è‰²è¡¨ï¼ˆä½ å¯è‡ªå®šä¹‰ï¼‰
COLOR_TABLE = [
    (255, 0, 0),    # Red
    (0, 255, 0),    # Green
    (0, 0, 255),    # Blue
    (255, 255, 0),  # Yellow
    (255, 0, 255),  # Magenta
    (0, 255, 255),  # Cyan
]

# ---------------- è¾…åŠ©å‡½æ•° ----------------
def apply_mask_color(mask, color, alpha=120):
    """mask: [H, W] np.uint8 0-255, color: (R,G,B), alpha: 0-255"""
    mask_img = Image.fromarray(mask)
    color_img = Image.new("RGBA", mask_img.size, color + (0,))
    # åªåœ¨å‰æ™¯ä¸ŠåŠ é€æ˜è‰²
    mask_rgba = mask_img.convert("L").point(lambda x: alpha if x > 0 else 0)
    color_img.putalpha(mask_rgba)
    return color_img

def prompt_tokens(prompt: str):
    """æŠŠ prompt æ‹†æˆé•¿åº¦â‰¥3 çš„è‹±æ–‡å•è¯åˆ—è¡¨ï¼Œå…¨å°å†™"""
    return [w for w in re.findall(r"[a-zA-Z']+", prompt.lower()) if len(w) >= 3]

def fuzzy_ratio(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def prompt_match(tokens, ocr_tokens, fuzzy_th=0.8):
    """
    æ‰€æœ‰ tokens å¿…é¡»å…¨éƒ¨åŒ¹é… OCR æ‰è¿”å› Trueï¼š
    â‘  è‹¥ kw æ˜¯æŸä¸ª OCR t çš„å­ä¸² â†’ åŒ¹é…æˆåŠŸ
    â‘¡ æˆ–æ¨¡ç³Šç›¸ä¼¼åº¦ â‰¥ fuzzy_th    â†’ åŒ¹é…æˆåŠŸ
    """
    matched_scores = []

    for kw in tokens:
        kw = kw.lower()
        matched = False
        best_score = 0.0

        for t in ocr_tokens:
            t = t.lower()
            if kw in t:
                matched = True
                best_score = 1.0
                break  # ç›´æ¥å‘½ä¸­ï¼Œé€€å‡ºå†…å±‚
            score = fuzzy_ratio(kw, t)
            if score >= fuzzy_th:
                matched = True
                best_score = max(best_score, score)

        if not matched:
            return False, 0  # åªè¦æœ‰ä¸€ä¸ªå…³é”®è¯ä¸åŒ¹é… â†’ å…¨éƒ¨å¤±è´¥
        matched_scores.append(best_score)

    # æ‰€æœ‰éƒ½åŒ¹é…äº†ï¼Œè¿”å›æœ€å°å¾—åˆ†ï¼ˆæœ€å¼±ä¸€é¡¹ï¼‰
    return True, min(matched_scores)

# --------------------

class SAMSegmenter:
    def __init__(self, sam_ckpt="src/VLM_agent/sam_hq/sam_vit_h_4b8939.pth", device="cpu"):
        self.device = device
        self.sam = sam_model_registry["vit_h"](checkpoint=sam_ckpt).to(self.device)
        self.sam_predictor = SamPredictor(self.sam)

    def segment_with_boxes(self, image_path, boxes_xyxy, multimask_output=False):
        """
        image_path      : str
        boxes_xyxy      : (x0,y0,x1,y1)  æˆ–  [(...), (...)]   åƒç´ åæ ‡
        returns         : numpy.bool_  [N,H,W]  (multi=False)  æˆ–  [N,3,H,W]
        """
        img_bgr = cv2.imread(image_path)
        if img_bgr is None:
            raise FileNotFoundError(image_path)
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        H, W = img_rgb.shape[:2]

        # --- æ ‡å‡†åŒ– boxes åˆ° [N,4] ---
        if isinstance(boxes_xyxy[0], (int, float)):
            boxes_arr = np.asarray([boxes_xyxy], dtype=np.float32)
        else:
            boxes_arr = np.asarray(boxes_xyxy, dtype=np.float32)

        self.sam_predictor.set_image(img_rgb)
        inp_boxes = torch.as_tensor(boxes_arr, device=self.device)           # [N,4]
        tr_boxes  = self.sam_predictor.transform.apply_boxes_torch(
                        inp_boxes, (H, W))

        masks, _, _ = self.sam_predictor.predict_torch(
            point_coords=None,
            point_labels=None,
            boxes=tr_boxes,
            multimask_output=multimask_output            # => [N,M,H,W]
        )

        masks = masks.bool().cpu().numpy()               # â†’ numpy
        if not multimask_output:                         # [N,1,H,W] â†’ [N,H,W]
            masks = masks[:, 0]

        return masks                   

class TextDrivenSegmenter:
    def __init__(self, fastsam_model_path='src/VLM_agent/FastSAM/FastSAM-x.pt', use_gpu=False):
        """Wrapper that loads OwlViT + (Fast)SAM.

        fastsam_model_path: Pfad zur FastSAM-x.pt. Falls nicht vorhanden wird versucht
        sie relativ zum aktuellen Dateiordner unter FastSAM/FastSAM-x.pt zu finden.
        """
        import os
        from pathlib import Path
        # Robustere Pfadbehandlung (Backslashes aus Windows-String vermeiden)
        fastsam_model_path = fastsam_model_path.replace('\\', '/')
        if not os.path.isfile(fastsam_model_path):
            alt = Path(__file__).parent / 'FastSAM' / 'FastSAM-x.pt'
            if alt.is_file():
                fastsam_model_path = str(alt)
            else:
                raise FileNotFoundError(
                    f"FastSAM Gewichte nicht gefunden: '{fastsam_model_path}' oder '{alt}'. Bitte Datei ablegen.")
        self.owl_processor = OwlViTProcessor.from_pretrained("google/owlvit-large-patch14")
        self.owl_model     = OwlViTForObjectDetection.from_pretrained("google/owlvit-large-patch14")
        
        self.fastsam       = YOLO(fastsam_model_path)
        self.sam = SAMSegmenter(sam_ckpt="src/VLM_agent/sam_hq/sam_vit_h_4b8939.pth")
        
        self.reader        = easyocr.Reader(['de'], gpu=use_gpu)
        self.device        = "cuda" if (use_gpu and torch.cuda.is_available()) else "cpu"
        self.owl_model.to(self.device)
        if self.device == "cuda":
            self.fastsam.to(self.device)
        self.fastsam_conf, self.fastsam_iou, self.fastsam_size = 0.4, 0.9, 1024
        self.color_map = {}
        trans_name = "Helsinki-NLP/opus-mt-en-de"
        self.trans_tok = MarianTokenizer.from_pretrained(trans_name)
        self.trans_mod = MarianMTModel.from_pretrained(trans_name)
        self.trans_mod.to(self.device)      # æ”¾åŒä¸€å¼  GPU / CPU

    @functools.lru_cache(maxsize=512)
    @torch.inference_mode()
    def _en_word2de(self, word: str) -> str:
        batch = self.trans_tok(word, return_tensors="pt").to(self.device)
        gen   = self.trans_mod.generate(**batch, max_length=16)
        return self.trans_tok.decode(gen[0], skip_special_tokens=True)

    # ---------------- ä¸»æµç¨‹ ----------------
    def detect_and_segment(self, image_path, text_prompts, text_label, multi_task=False, if_sam=True, if_translate=False):
        image = Image.open(image_path).convert("RGB")
        W, H  = image.size
        draw_img   = image.copy()
        all_boxes, all_masks, all_points = [], [], []
        not_match = False

        for prompt, label in zip (text_prompts, text_label):
            if label == "":
                not_match = True

            if if_translate:  # ç¿»è¯‘æˆå¾·è¯­
                tokens_en = prompt_tokens(label) 
                tokens_de = [self._en_word2de(w).lower() for w in tokens_en]
                p_tokens = tokens_de         # æ•´å¥ promptâ†’å•è¯åˆ—è¡¨
                print(f'Prompt EN: "{label}"   â†’   DE: "{tokens_de}"')
            else:
                p_tokens = prompt_tokens(label)
                print(f'Prompt: "{label}"   â†’   Tokens: {p_tokens}')
            
            inputs = self.owl_processor(text=[prompt], images=image,
                                        return_tensors="pt").to(self.device)
            with torch.no_grad():
                out = self.owl_model(**inputs)
            boxes  = out.pred_boxes[0].cpu()
            scores = out.logits[0].cpu().sigmoid()[:, 0]
            keep   = torch.topk(scores, k=min(5, len(scores))).indices  # å‰ 5 ä¸ªå€™é€‰

            matched = []   # OCR å‘½ä¸­çš„å€™é€‰
            for idx in keep:
                if scores[idx] < 0.1:    # åˆ†æ•°ä¸‹é™
                    continue
                if not_match:            # æ— åŒ¹é…ä»»åŠ¡ï¼Œç›´æ¥ç”¨æœ€é«˜åˆ†çš„æ¡†
                    box = self._box_xyxy(boxes[idx], W, H)
                    matched.append((box, scores[idx].item()))
                else:
                    box  = self._box_xyxy(boxes[idx], W, H)
                    x1,y1,x2,y2 = box
                    pad = 20                  # æ‰©å¼  20 px å…å¾—æˆªæ‰æ–‡å­—
                    crop = image.crop((max(0,x1-pad), max(0,y1-pad),
                                    min(W,x2+pad), min(H,y2+pad)))
                    ocr_tokens = [t.lower() for t in self.reader.readtext(np.array(crop),
                                                                        detail=0)]
                    exist , matched_score = prompt_match(p_tokens, ocr_tokens)
                    if exist:   # å‘½ä¸­ä¸€æ¬¡å³å¯
                        total_score = matched_score + scores[idx].item()  # å‘½ä¸­åˆ†æ•° + æ£€æµ‹åˆ†æ•°
                        print(matched_score, scores[idx].item(), total_score)
                        matched.append((box, total_score))
                
            
            if len(matched)>1 and not multi_task:
                # å¤šä»»åŠ¡æ¨¡å¼ä¸‹ï¼Œå…è®¸å¤šä¸ªåŒ¹é…ï¼›å¦åˆ™åªä¿ç•™æœ€é«˜åˆ†çš„ä¸€ä¸ª
                matched = sorted(matched, key=lambda x: x[1], reverse=True)[:1]

            # è‹¥æœ‰å‘½ä¸­ç”¨å‘½ä¸­ï¼Œå¦åˆ™ç”¨æœ€é«˜ç½®ä¿¡åº¦é‚£ä¸€æ¡†å…œåº•
            cand = matched if matched else [(self._box_xyxy(boxes[keep[0]], W, H),
                                             scores[keep[0]].item())]

            for box, conf in cand:
                
                if if_sam:
                    masks_sam = self.sam.segment_with_boxes(image_path, [box],
                                                            multimask_output=False)
                    mask = masks_sam[0].astype(np.uint8)             # å–ç¬¬ 1 ä¸ªæ¡†çš„å•ä¸€ mask
                else:
                    mask = self._fastsam_seg(image_path, box)

                bcen, mcen = self._centers(box, mask)
                all_boxes.append((box, prompt, conf))
                all_masks.append(mask)
                all_points.append({"target": prompt,
                                   "box_center_point": bcen,
                                   "seg_center_point": mcen})

        result = self._visual(draw_img, all_boxes, all_masks)
        return result, all_boxes, all_points


    # ---------- å·¥å…· ----------
    @staticmethod
    def _box_xyxy(box, W, H):
        cx, cy, bw, bh = box.numpy()
        return (int(max(0,(cx-bw/2)*W)), int(max(0,(cy-bh/2)*H)),
                int(min(W,(cx+bw/2)*W)), int(min(H,(cy+bh/2)*H)))

    def _centers(self, box, mask):
        x1,y1,x2,y2 = box
        cx, cy = (x1+x2)//2, (y1+y2)//2
        ys, xs = np.where(mask>0)
        return (cx,cy), (None,None) if len(xs)==0 else (int(xs.mean()),int(ys.mean()))

    def _fastsam_seg(self, img_path, box):
        image = Image.open(img_path).convert("RGB")
        W,H   = image.size
        x1,y1,x2,y2 = map(int, box)
        crop  = np.array(image.crop((x1,y1,x2,y2)))
        res   = self.fastsam.predict(crop, conf=self.fastsam_conf, iou=self.fastsam_iou,
                                     imgsz=self.fastsam_size, device=self.device, verbose=False)
        best,marea = None,-1
        for r in res:
            if r.masks is None: continue
            for m in r.masks.data:
                arr = m.cpu().numpy().astype(np.uint8)
                area = arr.sum()
                if area>marea: best,marea = arr,area
        final = np.zeros((H,W),dtype=np.uint8)
        if best is not None:
            if best.shape!=(y2-y1,x2-x1):
                best = cv2.resize(best,(x2-x1,y2-y1),interpolation=cv2.INTER_NEAREST)
            final[y1:y2,x1:x2] = best
        return final

    # ---------- å¯è§†åŒ– ----------
    def _visual(self, img, boxes, masks):
        draw = ImageDraw.Draw(img)
        overlay = Image.new("RGBA", img.size, (0,0,0,0))
        for (box,label,score),mask in zip(boxes,masks):
            if label not in self.color_map:
                self.color_map[label] = tuple(np.random.randint(64,256,size=3))
            col = self.color_map[label]
            draw.rectangle(box, outline=col, width=3)
            txt = f"{label} ({score:.2f})"
            try: font = ImageFont.truetype("arial.ttf", 20)
            except: font = ImageFont.load_default()
            draw.text((box[0], box[1]-25), txt, fill=col, font=font)
            if mask.any():
                rgba = np.zeros((*mask.shape,4),dtype=np.uint8)
                rgba[mask>0] = (*col,200)
                overlay = Image.alpha_composite(overlay, Image.fromarray(rgba))
        return Image.alpha_composite(img.convert("RGBA"), overlay).convert("RGB")

def find_object_central_pixel(target: str, text: str, image_path, is_sam: bool = True, if_translate: bool = False):
    # Verwende forward slashes (Linux) und robuste Lade-Logik in TextDrivenSegmenter
    seg = TextDrivenSegmenter(fastsam_model_path="src/VLM_agent/FastSAM/FastSAM-x.pt")
    img, boxes, points = seg.detect_and_segment(image_path, [target], [text],  multi_task = False, if_sam = is_sam, if_translate = if_translate)
    img.save("result.jpg")
    for (b,l,s),pt in zip(boxes, points):
        print(f"{l} @ {b}  conf={s:.2f}")
        print("   centers:", pt)  

    target_prompt = points[0]["target"]
    box_center_point = points[0]["box_center_point"]
    seg_center_point = points[0]["seg_center_point"]
    bbox = tuple(boxes[0][0])  # è·å–ç¬¬ä¸€ä¸ªç›®æ ‡çš„è¾¹ç•Œæ¡†
    score = boxes[0][2]  # è·å–ç¬¬ä¸€ä¸ªç›®æ ‡çš„ç½®ä¿¡åº¦åˆ†æ•°

    return target_prompt, box_center_point, seg_center_point, bbox, score   


# ---------------- demo ----------------
if __name__ == "__main__":
    target_label = "pepper bottle"  
    text = "Schwarzer Pfeffer ganz"
    # text = ""
    target_prompt, box_center_point, seg_center_point, bbox, score = find_object_central_pixel(target_label, text, is_sam = True, if_translate = False)  # è°ƒç”¨å‡½æ•°å¤„ç†å›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹
    print(f"ğŸ” Detected target: {target_label}")
    print(f"ğŸ“ Target prompt: {target_prompt}")
    print(f"ğŸ“ Bounding box: {bbox}")
    print(f"ğŸ¯ Box center point: {box_center_point}")
    print(f"ğŸ¯ Segmentation center point: {seg_center_point}")
    print(f"ğŸ“Š Detection score: {score}")




# scone
